# -*- coding: utf-8 -*-
"""word_extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18dy5NDw4OdR_PWnj5enxPCaXnQccDZs1
"""

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import time
import string
import spacy
from collections import Counter
import tensorflow as tf
import os
import matplotlib.pylab as plt
import spacy
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import word2vec
import string

class text_cleaning:
  #Cleaning the texts
  
  punct_regex = re.compile('([%s])' % (string.punctuation + '‘’'))
  spaces_regex = re.compile(r'\s{2,}')
  number_regex = re.compile(r'\d+')
  
  def __init__(self, string):
    self.string = string
    
  def remove_nonalpha(self):
    # lower the words, remove non-alphabetic words and extra blanks
    preprocessed = self.string.replace('\'', '')
    preprocessed = preprocessed.lower()
    preprocessed = self.punct_regex.sub(' ', preprocessed)
    preprocessed = self.number_regex.sub('_NUMBER_', preprocessed)    
    
    return preprocessed
  
  def remove_stopword(self):
    # remove the stopwords
    from spacy.lang.en import STOP_WORDS
    spacy_stopwords = STOP_WORDS
    
    return ' '.join([word for word in self.string.split() if word not in spacy_stopwords])

  
  def lemma_word(self):
    # lemmatize the words
    nlp = spacy.load('en_core_web_sm')
    
    return ' '.join([token.lemma_ for token in nlp(self.string)])
  
  def stem_word(self):
    # stem the words 
    stemmer = PorterStemmer()
    
    return ' '.join([stemmer.stem(token) for token in self.string.split()])

class word_extraction_tfidf:
  # extract the key words by using tfidf and bigram
  
  def __init__(self, df):
    self.df = df
    
  def key_words(self, column):
    # Use the tfidf to find top 10 words appearing in the seed patents. 
    #We assume those patents would include more synonyms to "purify water" or "filter water"
    tfidf = TfidfVectorizer(ngram_range=(1, 2))
    weights = tfidf.fit_transform(self.df[column])
    score = zip(tfidf.get_feature_names(),
              np.asarray(weights.sum(axis=0)).ravel())
    df_score = pd.DataFrame(score, columns=['feature', 'weight'])
    
    df_score['length'] = df_score['feature'].apply(lambda x: len(x.split()))
    df_score[df_score['length'] == 2][['feature', 'weight']].sort_values(by='weight', ascending=False).head(10)
    return df_score['feature'].tolist()

class word_extraction_word2vec:
  from word2vec import W2VModelDownload
  from word2vec import Word2Vec
  # extract the key words by using Word2Vec trained by Google Patents. 
  # Before running this program,  please include word2vec.py in your folder. 
  # You can download here: https://github.com/google/patents-public-data/blob/master/models/landscaping/word2vec.py
  # the model is stored here: https://console.cloud.google.com/storage/browser/patent_landscapes
  
  def __init__(self, bq_project):
    self.bq_project = bq_project
        
  def key_words(self, string, top_number=10):
  # use the pretrained model with 5.9 million pretrained model 
    model_name = '5.9m'
    model_download = W2VModelDownload(bq_project)
    model_download.download_w2v_model('patent_landscapes', model_name)
    word2vec5_9m = Word2Vec('5.9m')
    w2v_runtime = word2vec5_9m.restore_runtime()
    return w2v_runtime.find_similar(string, top_number)